{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dogs SGD opt.ipynb","provenance":[],"collapsed_sections":["oiHbxUL7X9NJ"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9641e637c8be47c2b96957e1bc19dff8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_396373cca8524a8e8e776618568d22db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4a7641491703457692c63d85bd05ec6a","IPY_MODEL_bd8ad9b8e5b742c0bb9f73994d933cca"]}},"396373cca8524a8e8e776618568d22db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a7641491703457692c63d85bd05ec6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6f3fdf3e3d994b51b9ada152a48a5d4d","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b6de8e4b674049c38c663c702403cf5e"}},"bd8ad9b8e5b742c0bb9f73994d933cca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_12c357f523b942d180c86281ca6222b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [05:37&lt;00:00, 304kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac16ced9c4864e528b813cd4c4fd5934"}},"6f3fdf3e3d994b51b9ada152a48a5d4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b6de8e4b674049c38c663c702403cf5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12c357f523b942d180c86281ca6222b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac16ced9c4864e528b813cd4c4fd5934":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GjubJNaMRU3a"},"source":["# Redes neuronales convolucionales**\n","**Proyecto: identificación de razas de perros**\n","\n","---\n","\n","### Roberto Alejandro Gutiérrez Guillén A01019608\n","### Emilio Hernández López A01336418\n","### Eduardo Badillo A01020716"]},{"cell_type":"markdown","metadata":{"id":"jgKPXsgmO2cF"},"source":["Daremos los primeros pasos para desarrollar un algoritmo que pueda usarse como parte de una aplicación web o móvil.\n","\n","Al final de este proyecto, su código aceptará cualquier imagen proporcionada por el usuario como entrada. Si se detecta un perro en la imagen, proporcionará una estimación de la raza del perro. Si se detecta un humano, proporcionará una estimación de la raza de perro que se parece más."]},{"cell_type":"markdown","metadata":{"id":"HHsb7BQEPEGa"},"source":["### El camino por delante\n","\n","Dividimos la notebook en pasos separados.\n","\n","    Paso 0: importar conjuntos de datos\n","    Paso 1: Detecta humanos\n","    Paso 2: detectar perros\n","    Paso 3: crea una CNN para clasificar las razas de perros (desde cero)\n","    Paso 4: Cree una CNN para clasificar las razas de perros (usando Transfer Learning)\n","    Paso 5: prueba tu algoritmo"]},{"cell_type":"markdown","metadata":{"id":"oiHbxUL7X9NJ"},"source":["### Paso 0: importar conjuntos de datos\n","\n","    Descarga el conjunto de datos del perro. Descomprima la carpeta y colóquela en el directorio de inicio de este proyecto, en la ubicación / dogImages.\n","  \n","\n","    Descarga el conjunto de datos humanos. Descomprima la carpeta y colóquela en el directorio de inicio, en location / lfw."]},{"cell_type":"code","metadata":{"id":"zUaRBkpqzs7l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606259221425,"user_tz":360,"elapsed":46687,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"0704b43c-62c9-4599-ab30-013afdfdd3e3"},"source":["import numpy as np\n","from glob import glob\n","\n","# download dog and human dataset\n","!mkdir dataset\n","!wget -O dataset/dog_dataset.zip https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n","!wget -O dataset/human_dataset.zip https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n","!unzip -q dataset/dog_dataset.zip\n","!unzip -q dataset/human_dataset.zip\n","\n","# load filenames for human and dog images\n","human_files = np.array(glob(\"lfw/*/*\"))\n","dog_files = np.array(glob(\"dogImages/*/*/*\"))\n","\n","# print number of images in each dataset\n","print('There are %d total human images.' % len(human_files))\n","print('There are %d total dog images.' % len(dog_files))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-11-24 23:06:16--  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n","Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.120.112\n","Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.120.112|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1132023110 (1.1G) [application/zip]\n","Saving to: ‘dataset/dog_dataset.zip’\n","\n","dataset/dog_dataset 100%[===================>]   1.05G  42.1MB/s    in 26s     \n","\n","2020-11-24 23:06:42 (41.0 MB/s) - ‘dataset/dog_dataset.zip’ saved [1132023110/1132023110]\n","\n","--2020-11-24 23:06:42--  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n","Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.112.184\n","Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.112.184|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 196739509 (188M) [application/zip]\n","Saving to: ‘dataset/human_dataset.zip’\n","\n","dataset/human_datas 100%[===================>] 187.62M  42.2MB/s    in 4.9s    \n","\n","2020-11-24 23:06:48 (38.2 MB/s) - ‘dataset/human_dataset.zip’ saved [196739509/196739509]\n","\n","There are 13233 total human images.\n","There are 8351 total dog images.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7qlnsQROYOR3"},"source":["### Paso 1: Detecta humanos\n","\n","En esta sección, usaremos la implementación de OpenCV de clasificadores en cascada basados ​​en características de Haar para detectar rostros humanos en imágenes.\n","\n","OpenCV proporciona muchos detectores de rostros previamente entrenados, almacenados como archivos XML en [github](https://github.com/opencv/opencv/tree/master/data/haarcascades)."]},{"cell_type":"code","metadata":{"id":"JOYx51zC0gjt","executionInfo":{"status":"ok","timestamp":1606259222462,"user_tz":360,"elapsed":23872,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["import cv2 \n","# returns \"True\" if face is detected in image stored at img_path\n","face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')\n","\n","def face_detector(img_path):\n","    img = cv2.imread(img_path)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(gray)\n","    return len(faces) > 0"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTT5G5EUY7uG"},"source":["### Evaluar el detector de rostro humano"]},{"cell_type":"code","metadata":{"id":"mB0_Zqv70gmL","executionInfo":{"status":"ok","timestamp":1606259225526,"user_tz":360,"elapsed":493,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["from tqdm import tqdm\n","\n","human_files_short = human_files[:100]\n","dog_files_short = dog_files[:100]\n","\n","def detect_face(images):\n","    cnt = 0\n","    for img in images:\n","        if face_detector(img):\n","            cnt += 1\n","    return cnt"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcprjLud0gpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606259258932,"user_tz":360,"elapsed":33355,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"c524231f-d412-4a88-e237-a319222945ba"},"source":["import cv2                \n","import matplotlib.pyplot as plt                        \n","%matplotlib inline \n","print(\"detect face in human_files: {} / {}\".format(detect_face(human_files_short), len(human_files_short)))\n","print(\"detect face in dog_files: {} / {}\".format(detect_face(dog_files_short), len(dog_files_short)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["detect face in human_files: 100 / 100\n","detect face in dog_files: 54 / 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BvvqKMxGReBd"},"source":["### Paso 2: detectar perros\n","\n","En esta sección, usamos un modelo previamente entrenado para detectar perros en imágenes.\n","\n","---\n","\n","Usaremos el modelo VGG-16 previamente entrenado\n","\n","La celda de código a continuación descarga el modelo VGG-16, junto con pesos que han sido entrenados en ImageNet, un conjunto de datos muy grande y muy popular que se utiliza para la clasificación de imágenes y otras tareas de visión. \n","\n","ImageNet contiene más de 10 millones de URL, cada una vinculada a una imagen que contiene un objeto de una de las 1000 categorías."]},{"cell_type":"code","metadata":{"id":"6isKBkDKRRDL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606259262281,"user_tz":360,"elapsed":36047,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"fe8fb082-aa6a-44c6-c0d5-f08512e14ee0"},"source":["import torch\n","import torchvision.models as models\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","print(\"cuda available? {0}\".format(use_cuda))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["cuda available? True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KW7Hy3kMafmS"},"source":["Antes de escribir la función, entra al siguiente [link](https://pytorch.org/docs/stable/torchvision/models.html) y tomen el tiempo para aprender cómo preprocesar adecuadamente los tensores para modelos previamente entrenados en la documentación de PyTorch.\n","\n"]},{"cell_type":"code","metadata":{"id":"zUJF1OuxVX_y","executionInfo":{"status":"ok","timestamp":1606262555997,"user_tz":360,"elapsed":4313,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["# define VGG16 model\n","VGG16 = models.vgg16(pretrained=True)\n","googlenet = models.googlenet(pretrained=True)\n","shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n","alexnet = models.alexnet(pretrained=True)\n","densenet = models.densenet161(pretrained=True)\n","resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n","\n","# move model to GPU if CUDA is available\n","if use_cuda:\n","    VGG16 = VGG16.cuda()\n","    googlenet = googlenet.cuda()\n","    shufflenet = shufflenet.cuda()\n","    alexnet = alexnet.cuda()\n","    densenet = densenet.cuda()\n","    resnext50_32x4d = resnext50_32x4d.cuda()"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9frChGR0s3P","executionInfo":{"status":"ok","timestamp":1606262560684,"user_tz":360,"elapsed":493,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["from PIL import Image\n","import torchvision.transforms as transforms\n","\n","\n","def load_image(img_path):    \n","    image = Image.open(img_path)\n","\n","    # resize to (244, 244) because VGG16 accept this shape\n","    in_transform = transforms.Compose(\n","                        [transforms.Resize(size=(244, 244)), transforms.ToTensor()])\n","\n","    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n","    image = in_transform(image)[:3,:,:].unsqueeze(0)\n","    return image"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"elxufEj1cnMM","executionInfo":{"status":"ok","timestamp":1606262561265,"user_tz":360,"elapsed":327,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def VGG16_predict(img_path):\n","    '''\n","    Use pre-trained VGG-16 model to obtain index corresponding to \n","    predicted ImageNet class for image at specified path\n","    \n","    Args:\n","        img_path: path to an image\n","        \n","    Returns:\n","        Index corresponding to VGG-16 model's prediction\n","    '''\n","    ## Load and pre-process an image from the given img_path\n","    ## Return the *index* of the predicted class for that image\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = VGG16(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIpy4QStc0rN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262562306,"user_tz":360,"elapsed":414,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"2cceccb7-6162-4f77-8b1c-ffef022f5ab9"},"source":["# predict dog using ImageNet class\n","VGG16_predict(dog_files_short[0])"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["151"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"Ke16zV3l1hZ3","executionInfo":{"status":"ok","timestamp":1606262562902,"user_tz":360,"elapsed":353,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def googlenet_predict(img_path):\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = googlenet(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6lBPvyr1hcI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262563610,"user_tz":360,"elapsed":420,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"e7628556-728e-4290-b313-ae413f42d02d"},"source":["googlenet_predict(dog_files_short[0])"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["515"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"eEl-nRbFWCZb","executionInfo":{"status":"ok","timestamp":1606262564280,"user_tz":360,"elapsed":605,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def shufflenet_predict(img_path):\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = shufflenet(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pK-p5prOWOGb","executionInfo":{"status":"ok","timestamp":1606262564569,"user_tz":360,"elapsed":444,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"142c259d-6a2a-4b09-b286-88774e760cec"},"source":["shufflenet_predict(dog_files_short[0])"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["845"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"JfLqpwripKlI","executionInfo":{"status":"ok","timestamp":1606262564899,"user_tz":360,"elapsed":470,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def alexnet_predict(img_path):\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = alexnet(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFpupqG8pKeo","executionInfo":{"status":"ok","timestamp":1606262565524,"user_tz":360,"elapsed":743,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"5127343c-7982-4c49-cd7b-e7f88c180460"},"source":["alexnet_predict(dog_files_short[0])"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["284"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"WySTMOG-pJom","executionInfo":{"status":"ok","timestamp":1606262565525,"user_tz":360,"elapsed":397,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def densenet_predict(img_path):\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = densenet(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K29oMjqXpJgo","executionInfo":{"status":"ok","timestamp":1606262566078,"user_tz":360,"elapsed":573,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"3b4dd35f-c858-425f-ebc3-ed25f9be4263"},"source":["densenet_predict(dog_files_short[0])"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["600"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"52vVXm4XpJVv","executionInfo":{"status":"ok","timestamp":1606262566337,"user_tz":360,"elapsed":449,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def resnext50_32x4d_predict(img_path):\n","    img = load_image(img_path)\n","    if use_cuda:\n","        img = img.cuda()\n","    ret = resnext50_32x4d(img)\n","    return torch.max(ret,1)[1].item() # predicted class index"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMs5VvxSpIbJ","executionInfo":{"status":"ok","timestamp":1606262566577,"user_tz":360,"elapsed":332,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"f95dd941-fe05-42de-cf86-8a63e3366413"},"source":["resnext50_32x4d_predict(dog_files_short[0])"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["463"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"f3da-kQj1hfn","executionInfo":{"status":"ok","timestamp":1606262567421,"user_tz":360,"elapsed":548,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["### returns \"True\" if a dog is detected in the image stored at img_path\n","def dog_detectorVG(img_path):\n","    pred = VGG16_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNZzeRB5XgCC","executionInfo":{"status":"ok","timestamp":1606262567945,"user_tz":360,"elapsed":393,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detectorGoogle(img_path):\n","    pred = googlenet_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y89GtwnxXhY6","executionInfo":{"status":"ok","timestamp":1606262568549,"user_tz":360,"elapsed":352,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detectorShuffle(img_path):\n","    pred = shufflenet_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gzudQ4wpv8C","executionInfo":{"status":"ok","timestamp":1606262569466,"user_tz":360,"elapsed":483,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detectorAlexnet(img_path):\n","    pred = alexnet_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnoO1rxfpv1a","executionInfo":{"status":"ok","timestamp":1606262571562,"user_tz":360,"elapsed":497,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detectorDensenet(img_path):\n","    pred = densenet_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"xG3XwK2kpvsz","executionInfo":{"status":"ok","timestamp":1606262572138,"user_tz":360,"elapsed":446,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detectorResnext50_32x4d(img_path):\n","    pred = resnext50_32x4d_predict(img_path)\n","    return pred >= 151 and pred <= 268 # true/false"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"M862OU0Y1koL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262573065,"user_tz":360,"elapsed":753,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"af244684-a5d4-48ce-da2a-532d89c46735"},"source":["print(\"\\n VG\")\n","print(dog_detectorVG(dog_files_short[0]))\n","print(dog_detectorVG(human_files_short[0]))\n","print(\"\\n Google\")\n","print(dog_detectorGoogle(dog_files_short[0]))\n","print(dog_detectorGoogle(human_files_short[0]))\n","print(\"\\n Shuffle\")\n","print(dog_detectorShuffle(dog_files_short[0]))\n","print(dog_detectorShuffle(human_files_short[0]))\n","print(\"\\n Alexnet\")\n","print(dog_detectorAlexnet(dog_files_short[0]))\n","print(dog_detectorAlexnet(human_files_short[0]))\n","print(\"\\n Densenet\")\n","print(dog_detectorDensenet(dog_files_short[0]))\n","print(dog_detectorDensenet(human_files_short[0]))\n","print(\"\\n Resnext50_32x4d\")\n","print(dog_detectorResnext50_32x4d(dog_files_short[0]))\n","print(dog_detectorResnext50_32x4d(human_files_short[0]))"],"execution_count":73,"outputs":[{"output_type":"stream","text":["\n"," VG\n","True\n","False\n","\n"," Google\n","False\n","False\n","\n"," Shuffle\n","False\n","False\n","\n"," Alexnet\n","False\n","False\n","\n"," Densenet\n","False\n","False\n","\n"," Resnext50_32x4d\n","False\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wuue8Q491kqq","executionInfo":{"status":"ok","timestamp":1606262582907,"user_tz":360,"elapsed":535,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["### Test the performance of the dog_detector function\n","### on the images in human_files_short and dog_files_short.\n","def dog_detector_testVG(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorVG(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"O96uBExyX28J","executionInfo":{"status":"ok","timestamp":1606262585117,"user_tz":360,"elapsed":456,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detector_testGoogle(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorGoogle(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLaR7wV0X3y_","executionInfo":{"status":"ok","timestamp":1606262585674,"user_tz":360,"elapsed":403,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detector_testShuffle(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorShuffle(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3kk__OEpAvg","executionInfo":{"status":"ok","timestamp":1606262586175,"user_tz":360,"elapsed":466,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detector_testAlexnet(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorAlexnet(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzmjmcUhpAo9","executionInfo":{"status":"ok","timestamp":1606262586481,"user_tz":360,"elapsed":383,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detector_testDensenet(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorDensenet(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzqtqu-4pAcn","executionInfo":{"status":"ok","timestamp":1606262587056,"user_tz":360,"elapsed":579,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def dog_detector_testResnext50_32x4d(files):\n","    cnt = 0\n","    for file in files:\n","        if dog_detectorResnext50_32x4d(file):\n","            cnt += 1\n","    return cnt, len(files)"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7CM0tb51ktm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262694751,"user_tz":360,"elapsed":49526,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"62004a7b-83fe-4592-d98c-4a1b77c02ef3"},"source":["print(\"\\n VG\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testVG(human_files_short)[0], dog_detector_testVG(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testVG(dog_files_short)[0], dog_detector_testVG(dog_files_short)[1]))\n","\n","print(\"\\n Google\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testGoogle(human_files_short)[0], dog_detector_testGoogle(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testGoogle(dog_files_short)[0], dog_detector_testGoogle(dog_files_short)[1]))\n","\n","print(\"\\n Shuffle\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testShuffle(human_files_short)[0], dog_detector_testShuffle(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testShuffle(dog_files_short)[0], dog_detector_testShuffle(dog_files_short)[1]))\n","\n","print(\"\\n Alex\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testAlexnet(human_files_short)[0], dog_detector_testAlexnet(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testAlexnet(dog_files_short)[0], dog_detector_testAlexnet(dog_files_short)[1]))\n","\n","print(\"\\n Densenet\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testDensenet(human_files_short)[0], dog_detector_testDensenet(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testDensenet(dog_files_short)[0], dog_detector_testDensenet(dog_files_short)[1]))\n","\n","print(\"\\n Resnext50_32x4d\")\n","print(\"detect a dog in human_files: {} / {}\".format(dog_detector_testResnext50_32x4d(human_files_short)[0], dog_detector_testResnext50_32x4d(human_files_short)[1]))\n","print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_testResnext50_32x4d(dog_files_short)[0], dog_detector_testResnext50_32x4d(dog_files_short)[1]))"],"execution_count":82,"outputs":[{"output_type":"stream","text":["\n"," VG\n","detect a dog in human_files: 0 / 100\n","detect a dog in dog_files: 90 / 100\n","\n"," Google\n","detect a dog in human_files: 3 / 100\n","detect a dog in dog_files: 7 / 100\n","\n"," Shuffle\n","detect a dog in human_files: 11 / 100\n","detect a dog in dog_files: 24 / 100\n","\n"," Alex\n","detect a dog in human_files: 0 / 100\n","detect a dog in dog_files: 48 / 100\n","\n"," Densenet\n","detect a dog in human_files: 0 / 100\n","detect a dog in dog_files: 0 / 100\n","\n"," Resnext50_32x4d\n","detect a dog in human_files: 0 / 100\n","detect a dog in dog_files: 0 / 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"37tPz2IHdEt5"},"source":["Sugerimos VGG-16 como una red potencial para detectar imágenes de perros en el algoritmo, pero pueden explorar otras redes previamente entrenadas (como Inception-v3, ResNet-50, etc.). \n","\n","Utilice la celda de código a continuación para probar otros modelos de PyTorch previamente entrenados. Realiza esta tarea opcional, muestra el rendimiento en human_files_short y dog_files_short."]},{"cell_type":"markdown","metadata":{"id":"NLPS0CtxdSI6"},"source":["### Paso 3: crea una CNN para clasificar las razas de perros (desde cero)\n","\n","Ahora que tenemos funciones para detectar humanos y perros en imágenes, necesitamos una forma de predecir la raza a partir de imágenes. En este paso, crearán una CNN que clasifica las razas de perros. \n","\n","---\n","Deben crear su CNN desde cero (por lo tanto, ¡todavía no puede usar el aprendizaje por transferencia!), Y debe lograr una precisión de prueba de al menos el 10%. En el paso 4, tendrán la oportunidad de utilizar el aprendizaje por transferencia para crear una CNN que alcance una precisión mucho mayor.\n","\n"]},{"cell_type":"code","metadata":{"id":"hIZOigiK1kwD","executionInfo":{"status":"ok","timestamp":1606259323476,"user_tz":360,"elapsed":11459,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["import os\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import torch\n","import numpy as np\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","## Specify appropriate transforms, and batch_sizes\n","\n","batch_size = 20\n","num_workers = 0\n","\n","data_dir = 'dogImages/'\n","train_dir = os.path.join(data_dir, 'train/')\n","valid_dir = os.path.join(data_dir, 'valid/')\n","test_dir = os.path.join(data_dir, 'test/')"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQzZVkd31kyp","executionInfo":{"status":"ok","timestamp":1606259323477,"user_tz":360,"elapsed":9870,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5bfThNQ1uKI","executionInfo":{"status":"ok","timestamp":1606259323478,"user_tz":360,"elapsed":9158,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["data_transforms = {'train': transforms.Compose([transforms.RandomResizedCrop(224),\n","                                     transforms.RandomHorizontalFlip(),\n","                                     transforms.ToTensor(),\n","                                     normalize]),\n","                   'val': transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     normalize]),\n","                   'test': transforms.Compose([transforms.Resize(size=(224,224)),\n","                                     transforms.ToTensor(), \n","                                     normalize])\n","                  }"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9JwPbWL1uMj","executionInfo":{"status":"ok","timestamp":1606259323478,"user_tz":360,"elapsed":8494,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["train_data = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n","valid_data = datasets.ImageFolder(valid_dir, transform=data_transforms['val'])\n","test_data = datasets.ImageFolder(test_dir, transform=data_transforms['test'])"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tNXJFY_1uR4","executionInfo":{"status":"ok","timestamp":1606259323479,"user_tz":360,"elapsed":8195,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["train_loader = torch.utils.data.DataLoader(train_data,\n","                                           batch_size=batch_size, \n","                                           num_workers=num_workers,\n","                                           shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data,\n","                                           batch_size=batch_size, \n","                                           num_workers=num_workers,\n","                                           shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_data,\n","                                           batch_size=batch_size, \n","                                           num_workers=num_workers,\n","                                           shuffle=False)\n","loaders_scratch = {\n","    'train': train_loader,\n","    'valid': valid_loader,\n","    'test': test_loader\n","}"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjkRdLyvdrO4"},"source":["Se ha aplicado RandomResizedCrop y RandomHorizontalFlip a los datos de entrenamiento. \n","\n","---\n","\n","Esto permite tener más imágenes usando técnicas de aumento de imagen. Generará más imágenes redimensionadas y volteadas. Mejorará el rendimiento del modelo y también ayudará a evitar el sobreajuste de los datos. Para los datos de validación, solo he aplicado las transformaciones de recorte de tamaño y centrado. Y, para los datos de prueba, solo se ha aplicado el cambio de tamaño de la imagen."]},{"cell_type":"markdown","metadata":{"id":"gMgN5Uv2d4EZ"},"source":["Crea una CNN para clasificar las razas de perros. Utilice la plantilla en la celda de código a continuación."]},{"cell_type":"code","metadata":{"id":"tSqKy2IX1uT-","executionInfo":{"status":"ok","timestamp":1606259323479,"user_tz":360,"elapsed":7460,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","num_classes = 133 # total classes of dog breeds"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhatlRna1uPx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606259323480,"user_tz":360,"elapsed":6681,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"50bc0aac-034d-47d2-d409-1b3ce3e0b9b2"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# define the CNN architecture\n","class Net(nn.Module):\n","    ### TODO: choose an architecture, and complete the class\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        ## Define layers of a CNN\n","        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","\n","        # pool\n","        self.pool = nn.MaxPool2d(2, 2)\n","        \n","        # fully-connected\n","        self.fc1 = nn.Linear(7 * 7 * 128, 512)\n","        self.fc2 = nn.Linear(512, num_classes) \n","        \n","        # drop-out\n","        self.dropout = nn.Dropout(0.3)\n","    \n","    def forward(self, x):\n","        ## Define forward behavior\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x) ##  Pool compresses the images \n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(x)\n","        \n","        # flatten\n","        x = x.view(-1, 7*7*128)\n","        \n","        x = self.dropout(x)\n","        x = F.relu(self.fc1(x))\n","        \n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","# instantiate the CNN\n","model_scratch = Net()\n","print(model_scratch)\n","\n","# move tensors to GPU if CUDA is available\n","if use_cuda:\n","    model_scratch.cuda()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=6272, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=133, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aPwf8q8oeDG3"},"source":["La primera capa de convolución tendrá un tamaño de kernel de 3 y stride 2, esto reducirá el tamaño de la imagen de entrada a la mitad. La segunda capa de convolución también tendrá el mismo tamaño, lo que reducirá el tamaño de la imagen de entrada a la mitad. La tercera capa de convolución tendrá un tamaño de kernel de 3.\n","\n","Se ha aplicado la combinación máxima de stride 2 después de cada capa de convolución para reducir el tamaño de la imagen a la mitad. También se aplica la activación de Relu para cada una de las capas de convolución.\n","\n","Luego, se \"aplanó\" (flattened) las entradas y se aplicó una capa de dropout con probabilidad de 0.3. Se aplican dos capas completamente conectadas con la activación de Relu y el dropout de 0.3 para producir el resultado final que predecirá las clases de razas de perros.\n","\n"]},{"cell_type":"code","metadata":{"id":"bxg5zG5511TY","executionInfo":{"status":"ok","timestamp":1606259323481,"user_tz":360,"elapsed":3219,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["import torch.optim as optim\n","\n","### select loss function\n","criterion_scratch = nn.CrossEntropyLoss()\n","\n","### select optimizer\n","optimizer_scratch = optim.SGD(model_scratch.parameters(), lr = 0.005)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JrF37Eb11Xy","executionInfo":{"status":"ok","timestamp":1606259330210,"user_tz":360,"elapsed":486,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path, last_validation_loss=None):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    if last_validation_loss is not None:\n","        valid_loss_min = last_validation_loss\n","    else:\n","        valid_loss_min = np.Inf\n","    \n","    for epoch in range(1, n_epochs+1):\n","        # initialize variables to monitor training and validation loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","        \n","        ###################\n","        # train the model #\n","        ###################\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(loaders['train']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()        \n","            # initialize weights to zero\n","            optimizer.zero_grad()\n","            \n","            output = model(data)\n","            \n","            # calculate loss\n","            loss = criterion(output, target)\n","            \n","            # back prop\n","            loss.backward()\n","            \n","            # grad\n","            optimizer.step()\n","            \n","            ## find the loss and update the model parameters accordingly\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            \n","            if batch_idx % 100 == 0:\n","                print('Epoch %d, Batch %d loss: %.6f' %\n","                  (epoch, batch_idx + 1, train_loss))\n","            \n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval()\n","        for batch_idx, (data, target) in enumerate(loaders['valid']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## update the average validation loss\n","            output = model(data)\n","            loss = criterion(output, target)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","\n","            \n","        # print training/validation statistics \n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","        \n","        if valid_loss < valid_loss_min:\n","            torch.save(model.state_dict(), save_path)\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_loss_min,\n","            valid_loss))\n","            valid_loss_min = valid_loss\n","            \n","    # return trained model\n","    return model"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca58DB3s11bP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262141024,"user_tz":360,"elapsed":2206472,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"fec340aa-f7e2-4a7f-edc4-cf2ffb3cf97e"},"source":["# train the model\n","model_scratch = train(30, loaders_scratch, model_scratch, optimizer_scratch, \n","                      criterion_scratch, use_cuda, 'model_scratch.pt')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Epoch 1, Batch 1 loss: 4.489655\n","Epoch 1, Batch 101 loss: 4.289608\n","Epoch 1, Batch 201 loss: 4.277520\n","Epoch 1, Batch 301 loss: 4.267110\n","Epoch: 1 \tTraining Loss: 4.268388 \tValidation Loss: 4.153804\n","Validation loss decreased (inf --> 4.153804).  Saving model ...\n","Epoch 2, Batch 1 loss: 4.301424\n","Epoch 2, Batch 101 loss: 4.230970\n","Epoch 2, Batch 201 loss: 4.225623\n","Epoch 2, Batch 301 loss: 4.219593\n","Epoch: 2 \tTraining Loss: 4.222711 \tValidation Loss: 4.097250\n","Validation loss decreased (4.153804 --> 4.097250).  Saving model ...\n","Epoch 3, Batch 1 loss: 3.992240\n","Epoch 3, Batch 101 loss: 4.160795\n","Epoch 3, Batch 201 loss: 4.161644\n","Epoch 3, Batch 301 loss: 4.146360\n","Epoch: 3 \tTraining Loss: 4.141198 \tValidation Loss: 3.897532\n","Validation loss decreased (4.097250 --> 3.897532).  Saving model ...\n","Epoch 4, Batch 1 loss: 4.183294\n","Epoch 4, Batch 101 loss: 4.081383\n","Epoch 4, Batch 201 loss: 4.099196\n","Epoch 4, Batch 301 loss: 4.098660\n","Epoch: 4 \tTraining Loss: 4.097754 \tValidation Loss: 3.950280\n","Epoch 5, Batch 1 loss: 4.237902\n","Epoch 5, Batch 101 loss: 4.056732\n","Epoch 5, Batch 201 loss: 4.054581\n","Epoch 5, Batch 301 loss: 4.060264\n","Epoch: 5 \tTraining Loss: 4.055528 \tValidation Loss: 3.837397\n","Validation loss decreased (3.897532 --> 3.837397).  Saving model ...\n","Epoch 6, Batch 1 loss: 3.869291\n","Epoch 6, Batch 101 loss: 3.989617\n","Epoch 6, Batch 201 loss: 3.985313\n","Epoch 6, Batch 301 loss: 3.996868\n","Epoch: 6 \tTraining Loss: 3.994901 \tValidation Loss: 3.791591\n","Validation loss decreased (3.837397 --> 3.791591).  Saving model ...\n","Epoch 7, Batch 1 loss: 3.958441\n","Epoch 7, Batch 101 loss: 3.968723\n","Epoch 7, Batch 201 loss: 3.958279\n","Epoch 7, Batch 301 loss: 3.964028\n","Epoch: 7 \tTraining Loss: 3.963763 \tValidation Loss: 3.751383\n","Validation loss decreased (3.791591 --> 3.751383).  Saving model ...\n","Epoch 8, Batch 1 loss: 3.834161\n","Epoch 8, Batch 101 loss: 3.910368\n","Epoch 8, Batch 201 loss: 3.927812\n","Epoch 8, Batch 301 loss: 3.925613\n","Epoch: 8 \tTraining Loss: 3.920528 \tValidation Loss: 3.800745\n","Epoch 9, Batch 1 loss: 3.990108\n","Epoch 9, Batch 101 loss: 3.828302\n","Epoch 9, Batch 201 loss: 3.848776\n","Epoch 9, Batch 301 loss: 3.875720\n","Epoch: 9 \tTraining Loss: 3.878546 \tValidation Loss: 3.636476\n","Validation loss decreased (3.751383 --> 3.636476).  Saving model ...\n","Epoch 10, Batch 1 loss: 3.900555\n","Epoch 10, Batch 101 loss: 3.832969\n","Epoch 10, Batch 201 loss: 3.824592\n","Epoch 10, Batch 301 loss: 3.824367\n","Epoch: 10 \tTraining Loss: 3.834459 \tValidation Loss: 3.676922\n","Epoch 11, Batch 1 loss: 3.928352\n","Epoch 11, Batch 101 loss: 3.743286\n","Epoch 11, Batch 201 loss: 3.763167\n","Epoch 11, Batch 301 loss: 3.769900\n","Epoch: 11 \tTraining Loss: 3.768107 \tValidation Loss: 3.609990\n","Validation loss decreased (3.636476 --> 3.609990).  Saving model ...\n","Epoch 12, Batch 1 loss: 3.390638\n","Epoch 12, Batch 101 loss: 3.686708\n","Epoch 12, Batch 201 loss: 3.694722\n","Epoch 12, Batch 301 loss: 3.722979\n","Epoch: 12 \tTraining Loss: 3.721332 \tValidation Loss: 3.564185\n","Validation loss decreased (3.609990 --> 3.564185).  Saving model ...\n","Epoch 13, Batch 1 loss: 3.479614\n","Epoch 13, Batch 101 loss: 3.683460\n","Epoch 13, Batch 201 loss: 3.690115\n","Epoch 13, Batch 301 loss: 3.699429\n","Epoch: 13 \tTraining Loss: 3.696122 \tValidation Loss: 3.551358\n","Validation loss decreased (3.564185 --> 3.551358).  Saving model ...\n","Epoch 14, Batch 1 loss: 3.787672\n","Epoch 14, Batch 101 loss: 3.598959\n","Epoch 14, Batch 201 loss: 3.664745\n","Epoch 14, Batch 301 loss: 3.666426\n","Epoch: 14 \tTraining Loss: 3.664162 \tValidation Loss: 3.610698\n","Epoch 15, Batch 1 loss: 3.209251\n","Epoch 15, Batch 101 loss: 3.571655\n","Epoch 15, Batch 201 loss: 3.617125\n","Epoch 15, Batch 301 loss: 3.607394\n","Epoch: 15 \tTraining Loss: 3.618369 \tValidation Loss: 3.599729\n","Epoch 16, Batch 1 loss: 3.233536\n","Epoch 16, Batch 101 loss: 3.557908\n","Epoch 16, Batch 201 loss: 3.563328\n","Epoch 16, Batch 301 loss: 3.575038\n","Epoch: 16 \tTraining Loss: 3.577753 \tValidation Loss: 3.490116\n","Validation loss decreased (3.551358 --> 3.490116).  Saving model ...\n","Epoch 17, Batch 1 loss: 3.561267\n","Epoch 17, Batch 101 loss: 3.552999\n","Epoch 17, Batch 201 loss: 3.553839\n","Epoch 17, Batch 301 loss: 3.571668\n","Epoch: 17 \tTraining Loss: 3.567455 \tValidation Loss: 3.358416\n","Validation loss decreased (3.490116 --> 3.358416).  Saving model ...\n","Epoch 18, Batch 1 loss: 3.488892\n","Epoch 18, Batch 101 loss: 3.487929\n","Epoch 18, Batch 201 loss: 3.476062\n","Epoch 18, Batch 301 loss: 3.485076\n","Epoch: 18 \tTraining Loss: 3.484677 \tValidation Loss: 3.432191\n","Epoch 19, Batch 1 loss: 3.380030\n","Epoch 19, Batch 101 loss: 3.457961\n","Epoch 19, Batch 201 loss: 3.487012\n","Epoch 19, Batch 301 loss: 3.487053\n","Epoch: 19 \tTraining Loss: 3.489085 \tValidation Loss: 3.417006\n","Epoch 20, Batch 1 loss: 4.008838\n","Epoch 20, Batch 101 loss: 3.427196\n","Epoch 20, Batch 201 loss: 3.431601\n","Epoch 20, Batch 301 loss: 3.438435\n","Epoch: 20 \tTraining Loss: 3.446632 \tValidation Loss: 3.315717\n","Validation loss decreased (3.358416 --> 3.315717).  Saving model ...\n","Epoch 21, Batch 1 loss: 3.145390\n","Epoch 21, Batch 101 loss: 3.353427\n","Epoch 21, Batch 201 loss: 3.369920\n","Epoch 21, Batch 301 loss: 3.396648\n","Epoch: 21 \tTraining Loss: 3.391086 \tValidation Loss: 3.312904\n","Validation loss decreased (3.315717 --> 3.312904).  Saving model ...\n","Epoch 22, Batch 1 loss: 3.438570\n","Epoch 22, Batch 101 loss: 3.304517\n","Epoch 22, Batch 201 loss: 3.347247\n","Epoch 22, Batch 301 loss: 3.358335\n","Epoch: 22 \tTraining Loss: 3.372643 \tValidation Loss: 3.440429\n","Epoch 23, Batch 1 loss: 4.071259\n","Epoch 23, Batch 101 loss: 3.281975\n","Epoch 23, Batch 201 loss: 3.320780\n","Epoch 23, Batch 301 loss: 3.329668\n","Epoch: 23 \tTraining Loss: 3.332016 \tValidation Loss: 3.330137\n","Epoch 24, Batch 1 loss: 2.993390\n","Epoch 24, Batch 101 loss: 3.320174\n","Epoch 24, Batch 201 loss: 3.304446\n","Epoch 24, Batch 301 loss: 3.310979\n","Epoch: 24 \tTraining Loss: 3.330204 \tValidation Loss: 3.272546\n","Validation loss decreased (3.312904 --> 3.272546).  Saving model ...\n","Epoch 25, Batch 1 loss: 3.909784\n","Epoch 25, Batch 101 loss: 3.248173\n","Epoch 25, Batch 201 loss: 3.267122\n","Epoch 25, Batch 301 loss: 3.276189\n","Epoch: 25 \tTraining Loss: 3.281221 \tValidation Loss: 3.402934\n","Epoch 26, Batch 1 loss: 3.158555\n","Epoch 26, Batch 101 loss: 3.235042\n","Epoch 26, Batch 201 loss: 3.262989\n","Epoch 26, Batch 301 loss: 3.252135\n","Epoch: 26 \tTraining Loss: 3.250860 \tValidation Loss: 3.268998\n","Validation loss decreased (3.272546 --> 3.268998).  Saving model ...\n","Epoch 27, Batch 1 loss: 3.466968\n","Epoch 27, Batch 101 loss: 3.144350\n","Epoch 27, Batch 201 loss: 3.203965\n","Epoch 27, Batch 301 loss: 3.210482\n","Epoch: 27 \tTraining Loss: 3.216846 \tValidation Loss: 3.223385\n","Validation loss decreased (3.268998 --> 3.223385).  Saving model ...\n","Epoch 28, Batch 1 loss: 3.192134\n","Epoch 28, Batch 101 loss: 3.232299\n","Epoch 28, Batch 201 loss: 3.228200\n","Epoch 28, Batch 301 loss: 3.205880\n","Epoch: 28 \tTraining Loss: 3.219455 \tValidation Loss: 3.279084\n","Epoch 29, Batch 1 loss: 3.099437\n","Epoch 29, Batch 101 loss: 3.087827\n","Epoch 29, Batch 201 loss: 3.146510\n","Epoch 29, Batch 301 loss: 3.179970\n","Epoch: 29 \tTraining Loss: 3.197368 \tValidation Loss: 3.352588\n","Epoch 30, Batch 1 loss: 3.238221\n","Epoch 30, Batch 101 loss: 3.078895\n","Epoch 30, Batch 201 loss: 3.135112\n","Epoch 30, Batch 301 loss: 3.167135\n","Epoch: 30 \tTraining Loss: 3.175076 \tValidation Loss: 3.155723\n","Validation loss decreased (3.223385 --> 3.155723).  Saving model ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hSMJUwL12LLr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262780429,"user_tz":360,"elapsed":424,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"16641826-800e-4641-c77f-b581eef36447"},"source":["# load the model that got the best validation accuracy\n","model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"pn3g7926_T23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262789706,"user_tz":360,"elapsed":8728,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"86e4b41e-71d5-4974-963f-9a78fb984ef8"},"source":["def test(loaders, model, criterion, use_cuda):\n","\n","    # monitor test loss and accuracy\n","    test_loss = 0.\n","    correct = 0.\n","    total = 0.\n","\n","    for batch_idx, (data, target) in enumerate(loaders['test']):\n","        # move to GPU\n","        if use_cuda:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the loss\n","        loss = criterion(output, target)\n","        # update average test loss \n","        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","        # convert output probabilities to predicted class\n","        pred = output.data.max(1, keepdim=True)[1]\n","        # compare predictions to true label\n","        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","        total += data.size(0)\n","            \n","    print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n","        100. * correct / total, correct, total))\n","\n","# call test function    \n","test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"],"execution_count":84,"outputs":[{"output_type":"stream","text":["Test Loss: 3.347345\n","\n","\n","Test Accuracy: 21% (183/836)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i7tr4n3egZhO"},"source":["### Paso 4: crea una CNN para clasificar las razas de perros (usando Transfer Learning)\n","\n","Ahora utilizarán el aprendizaje por transferencia para crear una CNN que pueda identificar la raza de perro a partir de imágenes. Su CNN debe alcanzar al menos un 60% de precisión en el equipo de prueba.\n","\n"]},{"cell_type":"code","metadata":{"id":"HLj976KO_T51","executionInfo":{"status":"ok","timestamp":1606262883252,"user_tz":360,"elapsed":558,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["loaders_transfer = loaders_scratch.copy()"],"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPmxH_hRgnyF"},"source":["Utilicen el aprendizaje por transferencia para crear una CNN para clasificar las razas de perros. Usen la celda de código a continuación y guarden su modelo inicializado como la variable model_transfer.\n","\n"]},{"cell_type":"code","metadata":{"id":"oUWMZvEB_T9Y","colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["9641e637c8be47c2b96957e1bc19dff8","396373cca8524a8e8e776618568d22db","4a7641491703457692c63d85bd05ec6a","bd8ad9b8e5b742c0bb9f73994d933cca","6f3fdf3e3d994b51b9ada152a48a5d4d","b6de8e4b674049c38c663c702403cf5e","12c357f523b942d180c86281ca6222b4","ac16ced9c4864e528b813cd4c4fd5934"]},"executionInfo":{"status":"ok","timestamp":1606262885444,"user_tz":360,"elapsed":1894,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"8e55987a-eba4-4ca2-9ef6-830074e6e3b2"},"source":["import torchvision.models as models\n","import torch.nn as nn\n","\n","model_transfer = models.resnet50(pretrained=True)"],"execution_count":86,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9641e637c8be47c2b96957e1bc19dff8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-lCXHmdZ_UBT","executionInfo":{"status":"ok","timestamp":1606262885445,"user_tz":360,"elapsed":1115,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["for param in model_transfer.parameters():\n","    param.requires_grad = False"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fYgB8Ck_UEN","executionInfo":{"status":"ok","timestamp":1606262885445,"user_tz":360,"elapsed":326,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["model_transfer.fc = nn.Linear(2048, 133, bias=True)"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL769h8y_T_s","executionInfo":{"status":"ok","timestamp":1606262886471,"user_tz":360,"elapsed":340,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["fc_parameters = model_transfer.fc.parameters()"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPo8NSoo_7CG","executionInfo":{"status":"ok","timestamp":1606262887086,"user_tz":360,"elapsed":415,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["for param in fc_parameters:\n","    param.requires_grad = True"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cz4wogCC_7Ep","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606262888966,"user_tz":360,"elapsed":479,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"72f53096-f0df-4930-ba69-dbf5dc2cc22a"},"source":["model_transfer"],"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=133, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"yW91hWLP_7Jo","executionInfo":{"status":"ok","timestamp":1606262902619,"user_tz":360,"elapsed":422,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["if use_cuda:\n","    model_transfer = model_transfer.cuda()"],"execution_count":93,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzvID8dPg2tH"},"source":["Se ha seleccionado el modelo previamente entrenado ResNet50 porque tiene un buen rendimiento en la clasificación de imágenes. La idea principal de este modelo se llama \"conexión de acceso directo de identidad\" que omite una o más capas. Esto nos permite evitar el sobreajuste durante el entrenamiento. Finalmente, se agrega una capa final completamente conectada que generará las probabilidades de 133 clases de razas de perros."]},{"cell_type":"markdown","metadata":{"id":"3pRPRn5RhBZV"},"source":["------\n","\n","Utilicen la siguiente celda de código para especificar una función de pérdida y un optimizador. Guarden la función de pérdida elegida como criticion_transfer y el optimizador como optimizer_transfer a continuación."]},{"cell_type":"code","metadata":{"id":"CQIgQHQ1_7Hv","executionInfo":{"status":"ok","timestamp":1606262904183,"user_tz":360,"elapsed":592,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["criterion_transfer = nn.CrossEntropyLoss()\n","optimizer_transfer = optim.SGD(model_transfer.fc.parameters(), lr=0.0005)"],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mulqeo-AAt7","executionInfo":{"status":"ok","timestamp":1606262904631,"user_tz":360,"elapsed":523,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["# train the model\n","# train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')\n","\n","def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf\n","    \n","    for epoch in range(1, n_epochs+1):\n","        # initialize variables to monitor training and validation loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","        \n","        ###################\n","        # train the model #\n","        ###################\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(loaders['train']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","\n","            # initialize weights to zero\n","            optimizer.zero_grad()\n","            \n","            output = model(data)\n","            \n","            # calculate loss\n","            loss = criterion(output, target)\n","            \n","            # back prop\n","            loss.backward()\n","            \n","            # grad\n","            optimizer.step()\n","            \n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            \n","            if batch_idx % 100 == 0:\n","                print('Epoch %d, Batch %d loss: %.6f' %\n","                  (epoch, batch_idx + 1, train_loss))\n","        \n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval()\n","        for batch_idx, (data, target) in enumerate(loaders['valid']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## update the average validation loss\n","            output = model(data)\n","            loss = criterion(output, target)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","\n","            \n","        # print training/validation statistics \n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","        \n","        ## TODO: save the model if validation loss has decreased\n","        if valid_loss < valid_loss_min:\n","            torch.save(model.state_dict(), save_path)\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_loss_min,\n","            valid_loss))\n","            valid_loss_min = valid_loss\n","            \n","    # return trained model\n","    return model"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"bf_AQVMvAAwn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606265242737,"user_tz":360,"elapsed":2337637,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"b5f021ac-2726-47a0-e99f-0392998363d1"},"source":["train(30, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')"],"execution_count":96,"outputs":[{"output_type":"stream","text":["Epoch 1, Batch 1 loss: 5.029637\n","Epoch 1, Batch 101 loss: 4.917074\n","Epoch 1, Batch 201 loss: 4.901573\n","Epoch 1, Batch 301 loss: 4.887914\n","Epoch: 1 \tTraining Loss: 4.880989 \tValidation Loss: 4.787185\n","Validation loss decreased (inf --> 4.787185).  Saving model ...\n","Epoch 2, Batch 1 loss: 4.856481\n","Epoch 2, Batch 101 loss: 4.788405\n","Epoch 2, Batch 201 loss: 4.771920\n","Epoch 2, Batch 301 loss: 4.757230\n","Epoch: 2 \tTraining Loss: 4.752950 \tValidation Loss: 4.651410\n","Validation loss decreased (4.787185 --> 4.651410).  Saving model ...\n","Epoch 3, Batch 1 loss: 4.755992\n","Epoch 3, Batch 101 loss: 4.670811\n","Epoch 3, Batch 201 loss: 4.662946\n","Epoch 3, Batch 301 loss: 4.649692\n","Epoch: 3 \tTraining Loss: 4.645695 \tValidation Loss: 4.525124\n","Validation loss decreased (4.651410 --> 4.525124).  Saving model ...\n","Epoch 4, Batch 1 loss: 4.592680\n","Epoch 4, Batch 101 loss: 4.570858\n","Epoch 4, Batch 201 loss: 4.556692\n","Epoch 4, Batch 301 loss: 4.543088\n","Epoch: 4 \tTraining Loss: 4.540322 \tValidation Loss: 4.397254\n","Validation loss decreased (4.525124 --> 4.397254).  Saving model ...\n","Epoch 5, Batch 1 loss: 4.508560\n","Epoch 5, Batch 101 loss: 4.458822\n","Epoch 5, Batch 201 loss: 4.457797\n","Epoch 5, Batch 301 loss: 4.450415\n","Epoch: 5 \tTraining Loss: 4.445443 \tValidation Loss: 4.272109\n","Validation loss decreased (4.397254 --> 4.272109).  Saving model ...\n","Epoch 6, Batch 1 loss: 4.459000\n","Epoch 6, Batch 101 loss: 4.375639\n","Epoch 6, Batch 201 loss: 4.366007\n","Epoch 6, Batch 301 loss: 4.353437\n","Epoch: 6 \tTraining Loss: 4.350399 \tValidation Loss: 4.150107\n","Validation loss decreased (4.272109 --> 4.150107).  Saving model ...\n","Epoch 7, Batch 1 loss: 4.279814\n","Epoch 7, Batch 101 loss: 4.294537\n","Epoch 7, Batch 201 loss: 4.284246\n","Epoch 7, Batch 301 loss: 4.266194\n","Epoch: 7 \tTraining Loss: 4.262269 \tValidation Loss: 4.051481\n","Validation loss decreased (4.150107 --> 4.051481).  Saving model ...\n","Epoch 8, Batch 1 loss: 4.058094\n","Epoch 8, Batch 101 loss: 4.172403\n","Epoch 8, Batch 201 loss: 4.175088\n","Epoch 8, Batch 301 loss: 4.168154\n","Epoch: 8 \tTraining Loss: 4.166189 \tValidation Loss: 3.922123\n","Validation loss decreased (4.051481 --> 3.922123).  Saving model ...\n","Epoch 9, Batch 1 loss: 4.143984\n","Epoch 9, Batch 101 loss: 4.122442\n","Epoch 9, Batch 201 loss: 4.093435\n","Epoch 9, Batch 301 loss: 4.080578\n","Epoch: 9 \tTraining Loss: 4.076997 \tValidation Loss: 3.818895\n","Validation loss decreased (3.922123 --> 3.818895).  Saving model ...\n","Epoch 10, Batch 1 loss: 3.817859\n","Epoch 10, Batch 101 loss: 4.029229\n","Epoch 10, Batch 201 loss: 4.014362\n","Epoch 10, Batch 301 loss: 4.000249\n","Epoch: 10 \tTraining Loss: 3.995116 \tValidation Loss: 3.719873\n","Validation loss decreased (3.818895 --> 3.719873).  Saving model ...\n","Epoch 11, Batch 1 loss: 3.654854\n","Epoch 11, Batch 101 loss: 3.935641\n","Epoch 11, Batch 201 loss: 3.924495\n","Epoch 11, Batch 301 loss: 3.910916\n","Epoch: 11 \tTraining Loss: 3.907951 \tValidation Loss: 3.608715\n","Validation loss decreased (3.719873 --> 3.608715).  Saving model ...\n","Epoch 12, Batch 1 loss: 3.921901\n","Epoch 12, Batch 101 loss: 3.840094\n","Epoch 12, Batch 201 loss: 3.836883\n","Epoch 12, Batch 301 loss: 3.825331\n","Epoch: 12 \tTraining Loss: 3.822284 \tValidation Loss: 3.497350\n","Validation loss decreased (3.608715 --> 3.497350).  Saving model ...\n","Epoch 13, Batch 1 loss: 3.759045\n","Epoch 13, Batch 101 loss: 3.746867\n","Epoch 13, Batch 201 loss: 3.747191\n","Epoch 13, Batch 301 loss: 3.745186\n","Epoch: 13 \tTraining Loss: 3.746749 \tValidation Loss: 3.395117\n","Validation loss decreased (3.497350 --> 3.395117).  Saving model ...\n","Epoch 14, Batch 1 loss: 3.521222\n","Epoch 14, Batch 101 loss: 3.666986\n","Epoch 14, Batch 201 loss: 3.669365\n","Epoch 14, Batch 301 loss: 3.665581\n","Epoch: 14 \tTraining Loss: 3.662818 \tValidation Loss: 3.330516\n","Validation loss decreased (3.395117 --> 3.330516).  Saving model ...\n","Epoch 15, Batch 1 loss: 3.627102\n","Epoch 15, Batch 101 loss: 3.614305\n","Epoch 15, Batch 201 loss: 3.609998\n","Epoch 15, Batch 301 loss: 3.600566\n","Epoch: 15 \tTraining Loss: 3.596427 \tValidation Loss: 3.205935\n","Validation loss decreased (3.330516 --> 3.205935).  Saving model ...\n","Epoch 16, Batch 1 loss: 3.693263\n","Epoch 16, Batch 101 loss: 3.540488\n","Epoch 16, Batch 201 loss: 3.523513\n","Epoch 16, Batch 301 loss: 3.511519\n","Epoch: 16 \tTraining Loss: 3.506554 \tValidation Loss: 3.131918\n","Validation loss decreased (3.205935 --> 3.131918).  Saving model ...\n","Epoch 17, Batch 1 loss: 3.390908\n","Epoch 17, Batch 101 loss: 3.426114\n","Epoch 17, Batch 201 loss: 3.444077\n","Epoch 17, Batch 301 loss: 3.461384\n","Epoch: 17 \tTraining Loss: 3.456661 \tValidation Loss: 3.016223\n","Validation loss decreased (3.131918 --> 3.016223).  Saving model ...\n","Epoch 18, Batch 1 loss: 3.295436\n","Epoch 18, Batch 101 loss: 3.400393\n","Epoch 18, Batch 201 loss: 3.391987\n","Epoch 18, Batch 301 loss: 3.380177\n","Epoch: 18 \tTraining Loss: 3.378575 \tValidation Loss: 2.964254\n","Validation loss decreased (3.016223 --> 2.964254).  Saving model ...\n","Epoch 19, Batch 1 loss: 3.503122\n","Epoch 19, Batch 101 loss: 3.340985\n","Epoch 19, Batch 201 loss: 3.344867\n","Epoch 19, Batch 301 loss: 3.331519\n","Epoch: 19 \tTraining Loss: 3.325937 \tValidation Loss: 2.887407\n","Validation loss decreased (2.964254 --> 2.887407).  Saving model ...\n","Epoch 20, Batch 1 loss: 3.477426\n","Epoch 20, Batch 101 loss: 3.245027\n","Epoch 20, Batch 201 loss: 3.246082\n","Epoch 20, Batch 301 loss: 3.244272\n","Epoch: 20 \tTraining Loss: 3.239369 \tValidation Loss: 2.813752\n","Validation loss decreased (2.887407 --> 2.813752).  Saving model ...\n","Epoch 21, Batch 1 loss: 3.068843\n","Epoch 21, Batch 101 loss: 3.219283\n","Epoch 21, Batch 201 loss: 3.197829\n","Epoch 21, Batch 301 loss: 3.182700\n","Epoch: 21 \tTraining Loss: 3.184623 \tValidation Loss: 2.710139\n","Validation loss decreased (2.813752 --> 2.710139).  Saving model ...\n","Epoch 22, Batch 1 loss: 2.971429\n","Epoch 22, Batch 101 loss: 3.110768\n","Epoch 22, Batch 201 loss: 3.125484\n","Epoch 22, Batch 301 loss: 3.122613\n","Epoch: 22 \tTraining Loss: 3.123633 \tValidation Loss: 2.680461\n","Validation loss decreased (2.710139 --> 2.680461).  Saving model ...\n","Epoch 23, Batch 1 loss: 3.086447\n","Epoch 23, Batch 101 loss: 3.081174\n","Epoch 23, Batch 201 loss: 3.084602\n","Epoch 23, Batch 301 loss: 3.073749\n","Epoch: 23 \tTraining Loss: 3.070433 \tValidation Loss: 2.582980\n","Validation loss decreased (2.680461 --> 2.582980).  Saving model ...\n","Epoch 24, Batch 1 loss: 3.325069\n","Epoch 24, Batch 101 loss: 3.056881\n","Epoch 24, Batch 201 loss: 3.034717\n","Epoch 24, Batch 301 loss: 3.015514\n","Epoch: 24 \tTraining Loss: 3.015750 \tValidation Loss: 2.527340\n","Validation loss decreased (2.582980 --> 2.527340).  Saving model ...\n","Epoch 25, Batch 1 loss: 2.761717\n","Epoch 25, Batch 101 loss: 2.967759\n","Epoch 25, Batch 201 loss: 2.956883\n","Epoch 25, Batch 301 loss: 2.940487\n","Epoch: 25 \tTraining Loss: 2.942112 \tValidation Loss: 2.464811\n","Validation loss decreased (2.527340 --> 2.464811).  Saving model ...\n","Epoch 26, Batch 1 loss: 2.857179\n","Epoch 26, Batch 101 loss: 2.910899\n","Epoch 26, Batch 201 loss: 2.919416\n","Epoch 26, Batch 301 loss: 2.901883\n","Epoch: 26 \tTraining Loss: 2.901797 \tValidation Loss: 2.392721\n","Validation loss decreased (2.464811 --> 2.392721).  Saving model ...\n","Epoch 27, Batch 1 loss: 2.735024\n","Epoch 27, Batch 101 loss: 2.862976\n","Epoch 27, Batch 201 loss: 2.859636\n","Epoch 27, Batch 301 loss: 2.854897\n","Epoch: 27 \tTraining Loss: 2.853252 \tValidation Loss: 2.350226\n","Validation loss decreased (2.392721 --> 2.350226).  Saving model ...\n","Epoch 28, Batch 1 loss: 3.003063\n","Epoch 28, Batch 101 loss: 2.821643\n","Epoch 28, Batch 201 loss: 2.814318\n","Epoch 28, Batch 301 loss: 2.810196\n","Epoch: 28 \tTraining Loss: 2.803596 \tValidation Loss: 2.266564\n","Validation loss decreased (2.350226 --> 2.266564).  Saving model ...\n","Epoch 29, Batch 1 loss: 2.632571\n","Epoch 29, Batch 101 loss: 2.775937\n","Epoch 29, Batch 201 loss: 2.763330\n","Epoch 29, Batch 301 loss: 2.763506\n","Epoch: 29 \tTraining Loss: 2.763716 \tValidation Loss: 2.208256\n","Validation loss decreased (2.266564 --> 2.208256).  Saving model ...\n","Epoch 30, Batch 1 loss: 2.528387\n","Epoch 30, Batch 101 loss: 2.723846\n","Epoch 30, Batch 201 loss: 2.725074\n","Epoch 30, Batch 301 loss: 2.714883\n","Epoch: 30 \tTraining Loss: 2.713840 \tValidation Loss: 2.152632\n","Validation loss decreased (2.208256 --> 2.152632).  Saving model ...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=133, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"cC9h5jERAA1y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606265242740,"user_tz":360,"elapsed":2334344,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"eb54d7d0-8562-4204-e1c4-838cc5136bbb"},"source":["# load the model that got the best validation accuracy\n","model_transfer.load_state_dict(torch.load('model_transfer.pt'))"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"zV9jLtToAA4j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606265254472,"user_tz":360,"elapsed":11704,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"9e6236fd-48ff-4212-cde6-13d8eba7fbd2"},"source":["test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Test Loss: 2.200948\n","\n","\n","Test Accuracy: 69% (578/836)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a12LesIFAA7O","executionInfo":{"status":"ok","timestamp":1606265346307,"user_tz":360,"elapsed":565,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["# list of class names by index, i.e. a name can be accessed like class_names[0]\n","class_names = [item[4:].replace(\"_\", \" \") for item in loaders_transfer['train'].dataset.classes]"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXYdeLnzAAzD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606265349359,"user_tz":360,"elapsed":650,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"6dc54b66-d878-4166-9a8b-caa2ceaf7093"},"source":["loaders_transfer['train'].dataset.classes[:10]"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['001.Affenpinscher',\n"," '002.Afghan_hound',\n"," '003.Airedale_terrier',\n"," '004.Akita',\n"," '005.Alaskan_malamute',\n"," '006.American_eskimo_dog',\n"," '007.American_foxhound',\n"," '008.American_staffordshire_terrier',\n"," '009.American_water_spaniel',\n"," '010.Anatolian_shepherd_dog']"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"code","metadata":{"id":"qLAm38HpBqke","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606265349677,"user_tz":360,"elapsed":714,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"2e875605-49b9-4fc4-c545-62203d808cb5"},"source":["class_names[:10]"],"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Affenpinscher',\n"," 'Afghan hound',\n"," 'Airedale terrier',\n"," 'Akita',\n"," 'Alaskan malamute',\n"," 'American eskimo dog',\n"," 'American foxhound',\n"," 'American staffordshire terrier',\n"," 'American water spaniel',\n"," 'Anatolian shepherd dog']"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"id":"dKM2XK4fBqnK","executionInfo":{"status":"ok","timestamp":1606265349677,"user_tz":360,"elapsed":564,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["from PIL import Image\n","import torchvision.transforms as transforms\n","\n","def load_input_image(img_path):    \n","    image = Image.open(img_path)\n","    prediction_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n","                                     transforms.ToTensor(), \n","                                     normalize])\n","\n","    # discard the transparent, alpha channel and add the batch dimension\n","    image = prediction_transform(image)[:3,:,:].unsqueeze(0)\n","    return image"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIpr8AEGBqp3","executionInfo":{"status":"ok","timestamp":1606265349678,"user_tz":360,"elapsed":381,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def predict_breed_transfer(model, class_names, img_path):\n","    # load the image and return the predicted breed\n","    img = load_input_image(img_path)\n","    model = model.cpu()\n","    model.eval()\n","    idx = torch.argmax(model(img))\n","    return class_names[idx]"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYPEM4P5hdFU"},"source":["### Paso 5: prueba tu algoritmo\n","\n","En esta sección, ¡probarán su nuevo algoritmo! ¿A qué tipo de perro cree el algoritmo que te pareces? Si tienes un perro, ¿predice con precisión la raza de tu perro? Si tienes un gato, ¿cree erróneamente que tu gato es un perro?"]},{"cell_type":"code","metadata":{"id":"nHGhL202CjDn","executionInfo":{"status":"ok","timestamp":1606265350447,"user_tz":360,"elapsed":551,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}}},"source":["def run_app(img_path):\n","    ## handle cases for a human face, dog, and neither\n","    img = Image.open(img_path)\n","    plt.imshow(img)\n","    plt.show()\n","    if dog_detector(img_path) is True:\n","        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n","        print(\"Dogs Detected!\\nIt looks like a {0}\".format(prediction))  \n","    elif face_detector(img_path) > 0:\n","        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n","        print(\"Hello, human!\\nIf you were a dog..You may look like a {0}\".format(prediction))\n","    else:\n","        print(\"Error! Can't detect anything..\")"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVkH3o4zCjGE","colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"status":"error","timestamp":1606089402022,"user_tz":360,"elapsed":2323160,"user":{"displayName":"Alejandro Guti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWnLJPn1IyZCJa7EVsOLQMVpm9XTtMPAl48pTbLow=s64","userId":"17643189198467322117"}},"outputId":"3ada767a-91b1-4c55-94de-00009457c3c5"},"source":["for img_file in os.listdir('/content/my_images'):\n","    img_path = os.path.join('/content/my_images', img_file)\n","    run_app(img_path)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-8c7138cefa4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/my_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/my_images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/my_images'"]}]}]}